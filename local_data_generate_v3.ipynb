{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c40f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from opencc import OpenCC\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from crawler.utils import Logger\n",
    "from curriculum_training.constants import (\n",
    "    USE_VLLM, ALLOW_VLLM, MAX_INPUT_LENGTH, MAX_NEW_TOKENS, DATASET_V3_DIR,\n",
    "    SUMMARY_FORMATTED_V3, SUMMARY_V3, ESSENTIALS_V3, NWR_V3\n",
    ")\n",
    "from news_with_rationale import NewsWithRationale as NWR\n",
    "from utils import load_udn_news\n",
    "from utils_vllm import (\n",
    "    LLM,\n",
    "    SamplingParams,\n",
    "    init_vllm_model,\n",
    "    filter_by_max_length,\n",
    "    vllm_batch_generate,\n",
    ")\n",
    "\n",
    "assert ALLOW_VLLM\n",
    "assert USE_VLLM\n",
    "\n",
    "MODELNAME = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "\n",
    "gen_logger = Logger(\"data_gen\", verbose_level=3)\n",
    "\n",
    "model: Optional[LLM] = None\n",
    "sampling_params: Optional[SamplingParams] = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELNAME)\n",
    "\n",
    "cc = OpenCC(\"s2twp\")  # Simplified Chinese to Traditional Chinese\n",
    "\n",
    "Message = list[dict[str, str]]\n",
    "\n",
    "if not os.path.exists(DATASET_V3_DIR):\n",
    "    os.makedirs(DATASET_V3_DIR)\n",
    "\n",
    "\n",
    "def essential_aspects_prompt(nwr: NWR) -> Message:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"請根據以下新聞內容以及摘要，提取新聞的關鍵要素與三元組，關鍵要素應為關鍵短句、名詞或事實，\"\n",
    "                \"三元組應為[實體1 | 關係 | 實體2]的格式，\"\n",
    "                \"這些三元組用於構成摘要，請用繁體中文回答。\"\n",
    "                \"請將每個關鍵要素與三元組用[]與、分隔。\"\n",
    "                \"例如：\"\n",
    "                \"關鍵要素：\\n[關鍵要素1]、[關鍵要素2]、[關鍵要素3]、...\\n\\n\"\n",
    "                \"三元組：\\n[實體1_1 | 關係_1 | 實體1_2]、[實體2_1 | 關係_2 | 實體2_2]、...\"\n",
    "                \"\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"新聞：\\n{nwr.article}\\n\\n摘要：\\n{nwr.summary}\\n\\n\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "def summary_prompt(nwr: NWR) -> Message:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"請根據以下新聞內容，為新聞生成一份100字內精簡的摘要，\"\n",
    "                \"請用繁體中文回答。\\n\"\n",
    "                \"例如：\\n\"\n",
    "                \"生成摘要：\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"新聞：\\n{nwr.article}\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "def local_gen_response(nwrs: list[NWR], prompt_fn: Callable) -> list[dict]:\n",
    "\n",
    "    assert model is not None\n",
    "    assert sampling_params is not None\n",
    "\n",
    "    data: list[dict] = []\n",
    "\n",
    "    if len(nwrs) == 0:\n",
    "        gen_logger.warning(\"No data need to generate.\")\n",
    "        return data\n",
    "\n",
    "    prompts: list[str] = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            prompt_fn(nwr),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for nwr in nwrs\n",
    "    ]\n",
    "\n",
    "    # Filter out prompts that are too long\n",
    "    prompts, _nwrs = filter_by_max_length(MAX_INPUT_LENGTH, prompts, nwrs)\n",
    "\n",
    "    if len(prompts) == 0:\n",
    "        return data\n",
    "\n",
    "    responses = vllm_batch_generate(model, prompts, sampling_params)\n",
    "    outputs = [response.outputs[0].text for response in responses]\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": nwr.id,\n",
    "            \"news\": nwr.article,\n",
    "            \"response\": response\n",
    "        }\n",
    "        for nwr, response in zip(_nwrs, outputs)\n",
    "    ]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_essential_and_triple(response: dict) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Parse the essential aspects from the response.\n",
    "    \"\"\"\n",
    "    pattern = (\n",
    "        r\"(?:關鍵要素：\\s*)?((?:\\[[^\\[\\]]+\\]、?)+)\\n*\"\n",
    "        r\"(?:三元組：\\s*)?((?:\\[[^\\[\\]]+\\]、?)+)\"\n",
    "    )\n",
    "    match = re.search(pattern, response[\"response\"])\n",
    "    if match:\n",
    "        content = match.group(1)\n",
    "        essentials = [\n",
    "            item.strip(\"[]\") for item in content.split(\"、\") if item\n",
    "        ]\n",
    "        content = match.group(2)\n",
    "        tripples = [\n",
    "            item.strip(\"[]\") for item in content.split(\"、\") if item\n",
    "        ]\n",
    "        return essentials, tripples\n",
    "\n",
    "    return [], []\n",
    "\n",
    "\n",
    "def parse_summary(response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Parse the summary from the response.\n",
    "    \"\"\"\n",
    "    pattern = r\"(?:生成摘要：\\s*)?(.+)\"\n",
    "    match = re.search(pattern, response[\"response\"].strip())\n",
    "    if match:\n",
    "        summary = match.group(1).strip()\n",
    "        return summary\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_summaries(nwrs: list[NWR], responses: list[dict]) -> list[NWR]:\n",
    "    \"\"\"\n",
    "    Parse the summaries from the responses and update the corresponding\n",
    "    NWR objects.\n",
    "    \"\"\"\n",
    "    response_map = {response[\"id\"]: response for response in responses}\n",
    "\n",
    "    for nwr in nwrs:\n",
    "        response = response_map.get(nwr.id)\n",
    "        if not response:\n",
    "            # gen_logger.warning(f\"No response found for NWR with id {nwr.id}\")\n",
    "            continue\n",
    "\n",
    "        summary = parse_summary(response)\n",
    "        if summary:\n",
    "            summary = cc.convert(summary)\n",
    "            nwr.summary = summary\n",
    "            nwr.rationale_summary = summary\n",
    "\n",
    "    return nwrs\n",
    "\n",
    "\n",
    "def parse_esss_and_tris(nwrs: list[NWR], responses: list[dict]) -> list[NWR]:\n",
    "    \"\"\"\n",
    "    Parse the essential aspects and triples from the responses\n",
    "    and update the corresponding NWR objects.\n",
    "    \"\"\"\n",
    "    response_map = {response[\"id\"]: response for response in responses}\n",
    "\n",
    "    for nwr in nwrs:\n",
    "        response = response_map.get(nwr.id)\n",
    "        if not response:\n",
    "            # gen_logger.warning(f\"No response found for NWR with id {nwr.id}\")\n",
    "            continue\n",
    "\n",
    "        essentials, triples = parse_essential_and_triple(response)\n",
    "        if essentials:\n",
    "            essentials = [cc.convert(ess) for ess in essentials]\n",
    "            nwr.essential_aspects = essentials\n",
    "        if triples:\n",
    "            triples = [cc.convert(tri) for tri in triples]\n",
    "            nwr.triples = triples\n",
    "\n",
    "    return nwrs\n",
    "\n",
    "\n",
    "def load_data(filename: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load the data from the file.\n",
    "    \"\"\"\n",
    "    data: list[dict] = []\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    except FileNotFoundError:\n",
    "        gen_logger.warning(f\"{filename} not found, starting from scratch.\")\n",
    "\n",
    "    gen_logger.info(f\"Loaded {len(data)} data from {filename}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_essentials_and_triples() -> tuple[\n",
    "    dict[int, list[str]], dict[int, list[str]]\n",
    "]:\n",
    "    data = load_data(ESSENTIALS_V3)\n",
    "    essential_aspects: dict[int, list[str]] = {}\n",
    "    triples: dict[int, list[str]] = {}\n",
    "    for dat in data:\n",
    "        if dat[\"id\"] in essential_aspects:\n",
    "            gen_logger.warning(f\"Duplicated essential id: {dat['id']}\")\n",
    "            continue\n",
    "        parsed_essentials, parsed_triples = parse_essential_and_triple(dat)\n",
    "        essential_aspects[dat[\"id\"]] = parsed_essentials\n",
    "        triples[dat[\"id\"]] = parsed_triples\n",
    "    return essential_aspects, triples\n",
    "\n",
    "\n",
    "def load_summary(filename) -> dict[int, str]:\n",
    "    data = load_data(filename)\n",
    "    summaries: dict[int, str] = {}\n",
    "    for dat in data:\n",
    "        if dat[\"id\"] in summaries:\n",
    "            gen_logger.warning(f\"Duplicated summary id: {dat['id']}\")\n",
    "            continue\n",
    "        summaries[dat[\"id\"]] = parse_summary(dat)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def get_ids_from_file(filename: str) -> set[int]:\n",
    "    \"\"\"\n",
    "    Get the ids from the file.\n",
    "    \"\"\"\n",
    "    ids: set[int] = set()\n",
    "    data = load_data(filename)\n",
    "    for dat in data:\n",
    "        if dat[\"id\"] in ids:\n",
    "            gen_logger.warning(f\"Duplicated id: {dat['id']}\")\n",
    "            continue\n",
    "        ids.add(dat[\"id\"])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def get_finished_ids() -> tuple[set[int], set[int], set[int], set[int]]:\n",
    "    \"\"\"\n",
    "    Get the finished essential/triple/summary/nwr ids from the generated files.\n",
    "    \"\"\"\n",
    "\n",
    "    # find finished essential ids\n",
    "    essential_ids = get_ids_from_file(ESSENTIALS_V3)\n",
    "    triple_ids = essential_ids.copy()\n",
    "\n",
    "    # find finished zh-tw ids\n",
    "    summary_ids = get_ids_from_file(SUMMARY_FORMATTED_V3)\n",
    "\n",
    "    nwr_ids: set[int] = essential_ids & triple_ids & summary_ids\n",
    "\n",
    "    return essential_ids, triple_ids, summary_ids, nwr_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, sampling_params = init_vllm_model(\n",
    "    model_name=MODELNAME,\n",
    "    max_input_length=MAX_INPUT_LENGTH,\n",
    "    max_new_tokens=MAX_NEW_TOKENS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86979328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the news\n",
    "news_list: list[str] = load_udn_news()\n",
    "# news_list = news_list[:10]\n",
    "gen_logger.info(f\"Loaded {len(news_list)} news\")\n",
    "\n",
    "# load finished ids\n",
    "essential_ids, triple_ids, summary_ids, nwr_ids = get_finished_ids()\n",
    "gen_logger.info(f\"Finished essential count: {len(essential_ids)}\")\n",
    "gen_logger.info(f\"Finished triple count: {len(triple_ids)}\")\n",
    "gen_logger.info(f\"Finished summary count: {len(summary_ids)}\")\n",
    "gen_logger.info(f\"Finished NWR count: {len(nwr_ids)}\")\n",
    "\n",
    "essential_data, triple_data = load_essentials_and_triples()\n",
    "summary_data = load_summary(SUMMARY_FORMATTED_V3)\n",
    "\n",
    "# assert len(essential_data) == len(list(essential_ids))\n",
    "# assert len(triple_data) == len(list(triple_ids))\n",
    "# assert len(summary_data) == len(list(summary_ids))\n",
    "\n",
    "nwrs = [NWR(news, id=i) for i, news in enumerate(news_list)]\n",
    "for nwr in nwrs:\n",
    "    if nwr.id in essential_ids:\n",
    "        nwr.essential_aspects = [\n",
    "            cc.convert(ess) for ess in essential_data[nwr.id]\n",
    "        ]\n",
    "    if nwr.id in triple_ids:\n",
    "        nwr.triples = [cc.convert(tri) for tri in triple_data[nwr.id]]\n",
    "    if nwr.id in summary_ids:\n",
    "        nwr.summary = cc.convert(summary_data[nwr.id])\n",
    "        nwr.rationale_summary = nwr.summary\n",
    "\n",
    "# with open(NWR_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for nwr in nwrs:\n",
    "#         if nwr.id in nwr_ids:\n",
    "#             f.write(json.dumps(nwr.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "gen_logger.info(f\"Loaded {len(nwr_ids)} NWRs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary responses and parse them\n",
    "_nwrs = [nwr for nwr in nwrs if nwr.id not in summary_ids]\n",
    "gen_logger.info(f\"Generating {len(_nwrs)} summaries\")\n",
    "\n",
    "responses = local_gen_response(_nwrs, summary_prompt)\n",
    "nwrs = parse_summaries(nwrs, responses)  # update the NWRs\n",
    "gen_logger.info(f\"Generated {len(responses)} summary responses\")\n",
    "\n",
    "with open(SUMMARY_V3, \"a\", encoding=\"utf-8\") as f:\n",
    "    for dat in responses:\n",
    "        f.write(json.dumps(dat, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "gen_logger.info(f\"{len([nwr for nwr in nwrs if nwr.summary == ''])} \"\n",
    "                f\"NWRs could not generate summaries\")\n",
    "\n",
    "# remove the NWRs that could not generate summaries\n",
    "nwrs = [nwr for nwr in nwrs if nwr.summary != \"\"]\n",
    "gen_logger.info(f\"remaining {len(nwrs)} NWRs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate essential responses and parse them\n",
    "_nwrs = [nwr for nwr in nwrs if nwr.id not in essential_ids]\n",
    "gen_logger.info(f\"Generating {len(_nwrs)} essential aspects\")\n",
    "responses = local_gen_response(_nwrs, essential_aspects_prompt)\n",
    "nwrs = parse_esss_and_tris(nwrs, responses)  # update the NWRs\n",
    "gen_logger.info(f\"Generated {len(responses)} essential and triples\")\n",
    "\n",
    "with open(ESSENTIALS_V3, \"a\", encoding=\"utf-8\") as f:\n",
    "    for dat in responses:\n",
    "        f.write(json.dumps(dat, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# remove the NWRs that could not generate essential aspects\n",
    "gen_logger.info(\n",
    "    f\"{len([nwr for nwr in nwrs if nwr.essential_aspects == []])} \"\n",
    "    f\"NWRs could not generate essential aspects\"\n",
    ")\n",
    "nwrs = [nwr for nwr in nwrs if nwr.essential_aspects != []]\n",
    "gen_logger.info(f\"remaining {len(nwrs)} NWRs\")\n",
    "\n",
    "# # save the NWRs to file\n",
    "# with open(NWR_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for nwr in nwrs:\n",
    "#         f.write(json.dumps(nwr.to_dict(), ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# import ollama\n",
    "import re\n",
    "from opencc import OpenCC\n",
    "from ollama import chat\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from crawler.utils import Logger\n",
    "from curriculum_training.constants import (\n",
    "    MAX_INPUT_LENGTH,\n",
    "    USE_VLLM,\n",
    "    ALLOW_VLLM,\n",
    "    NWR_FORMATTED_V3,\n",
    "    SUMMARY_V3,\n",
    "    ESSENTIALS_V3,\n",
    ")\n",
    "from news_with_rationale import NewsWithRationale as NWR\n",
    "from utils import int_set_str\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from utils_vllm import (\n",
    "    filter_by_max_length,\n",
    "    vllm_batch_generate,\n",
    ")\n",
    "\n",
    "assert ALLOW_VLLM\n",
    "\n",
    "FORMAT_MODEL = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "FORMAT_MODEL_OLLAMA = \"qwen2.5:32b-instruct\"\n",
    "\n",
    "\n",
    "logger = Logger(\"data_format\")\n",
    "cc = OpenCC(\"s2twp\")  # Simplified Chinese to Traditional Chinese\n",
    "\n",
    "ESS_START = len(\"關鍵要素：\\n[\")\n",
    "TRI_START = len(\"\\n[\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17702867",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_ess_tri_sys_prompt() -> str:\n",
    "    \"\"\" Get the format system prompt for the model. \"\"\"\n",
    "    return (\n",
    "        # \"請完成以下任務：\\n\"\n",
    "        \"請將以下內容轉換為關鍵要素和三元組的格式：\\n\"\n",
    "        \"1. 你會收到若干個關鍵要素，請將每個要素以[]與、分隔，並移除不必要的符號，如 '1.'、'；'等。\\n\"\n",
    "        \"2. 你會收到若干個三元組，請將其以頓號分隔，並移除不必要的符號，如 '1.'、'；'等。\\n\"\n",
    "        \"範例：\\n\"\n",
    "        \"關鍵要素：\\n\"\n",
    "        \"[關鍵要素1]、[關鍵要素2]、[關鍵要素3]...\\n\"\n",
    "        \"三元組：\\n\"\n",
    "        \"[三元組1_1 | 三元組1_2 | 三元組1_3]、[三元組2_1 | 三元組2_2 | 三元組2_3]...\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_ess_tri_user_prompt(nwr: NWR) -> str:\n",
    "    \"\"\"\n",
    "    Get the format system/user prompt for the given NewsWithRationale object.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"請將以下內容轉換為指定格式：\\n\"\n",
    "        \"關鍵要素：\\n\"\n",
    "        f\"{nwr.essential_aspects_str()}\\n\"\n",
    "        \"三元組：\\n\"\n",
    "        f\"{nwr.triples_str()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_summ_sys_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Get the format system prompt for the model.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"你會收到一個新聞文章以及其摘要，請評估該摘要是否為良好的摘要。\\n\"\n",
    "        \"若摘要良好（符合且文章內容），請輸出\\\"符合\\\"並將無關的內容移除。\\n\"\n",
    "        \"若摘要出現重複、雜亂的訊息、有未完成的句子，或是摘要本身不完整，請輸出\\\"不符合\\\"。\\n\"\n",
    "        \"範例輸入：\\n\"\n",
    "        \"新聞：\\n\"\n",
    "        \"新聞內容\\n\\n\"\n",
    "        \"摘要：\\n\"\n",
    "        \"摘要內容：\\n\\n\"\n",
    "        \"範例輸出：\\n\"\n",
    "        \"符合\\n\"\n",
    "        \"乾淨版摘要\\n\\n\"\n",
    "        \"或者\\n\"\n",
    "        \"不符合\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_summ_user_prompt(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Get the format user prompt for the given NewsWithRationale object.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"請評估以下內容的摘要是否是良好的新聞摘要：\\n\"\n",
    "        \"新聞：\\n\"\n",
    "        f\"{data['news']}\\n\\n\"\n",
    "        \"摘要：\\n\"\n",
    "        f\"{data['response']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def process_summ_ollama(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Process the given NewsWithRationale object and return the formatted string.\n",
    "    \"\"\"\n",
    "    sys_prompt = format_summ_sys_prompt()\n",
    "    user_prompt = format_summ_user_prompt(data)\n",
    "\n",
    "    # Generate the response using the model from Ollama\n",
    "    gen_response = chat(\n",
    "        model=FORMAT_MODEL_OLLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "    assert gen_response.message.content is not None\n",
    "\n",
    "    return gen_response.message.content\n",
    "\n",
    "\n",
    "def get_finished_id(filename) -> set[int]:\n",
    "    \"\"\" Get the finished news/NWR/zh-tw ids from the generated files. \"\"\"\n",
    "    finished_ids = set()\n",
    "    line_bak: str\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                line_bak = line\n",
    "                finished_ids.add(data[\"id\"])\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"{filename} not found, starting from scratch.\")\n",
    "        pass\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Error decoding JSON in line: {line_bak}.\")\n",
    "        exit()\n",
    "    return finished_ids\n",
    "\n",
    "\n",
    "def read_summ_file(filename: str, finished_ids=None) -> list[dict]:\n",
    "    \"\"\" Read the summary objects from the given file. \"\"\"\n",
    "    if finished_ids is None:\n",
    "        finished_ids = set()\n",
    "    summ_data: list[dict] = []\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if data[\"id\"] in finished_ids:\n",
    "                    continue\n",
    "                summ_data.append(data)\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"{filename} not found, starting from scratch.\")\n",
    "        pass\n",
    "    return summ_data\n",
    "\n",
    "\n",
    "def read_ess_tri_file(filename: str, finished_ids=None) -> list[dict]:\n",
    "    \"\"\" Read the essentials and triples from the given file. \"\"\"\n",
    "    if finished_ids is None:\n",
    "        finished_ids = set()\n",
    "    ess_tri_data: list[dict] = []\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                if data[\"id\"] in finished_ids:\n",
    "                    continue\n",
    "                ess_tri_data.append(data)\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"{filename} not found, starting from scratch.\")\n",
    "        pass\n",
    "    return ess_tri_data\n",
    "\n",
    "\n",
    "def extract_essentials_and_triples(\n",
    "    response: str\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    # Extract the essential aspects and triples from the response\n",
    "    formatted_response = cc.convert(response)\n",
    "\n",
    "    try:\n",
    "        ess_str, tri_str = formatted_response.split(\"三元組：\")\n",
    "        new_essentials = ess_str.split(\"]、[\")\n",
    "        new_essentials[0] = new_essentials[0][ESS_START:]\n",
    "        new_essentials[-1] = new_essentials[-1][:-3]  # Remove \\n\\n\n",
    "\n",
    "        new_triples = tri_str.split(\"]、[\")\n",
    "        new_triples[0] = new_triples[0][TRI_START:]\n",
    "        new_triples[-1] = new_triples[-1][:-1]\n",
    "\n",
    "    except ValueError:\n",
    "        logger.error(f\"Invalid response format: {response}\")\n",
    "        exit()\n",
    "\n",
    "    return new_essentials, new_triples\n",
    "\n",
    "\n",
    "def extract_summ(response: str) -> str:\n",
    "    # Extract the summary from the response\n",
    "    response = cc.convert(response)\n",
    "\n",
    "    if response.startswith(\"生成摘要：\\n\"):\n",
    "        summary = response[len(\"生成摘要：\\n\") :].strip()\n",
    "        return summary\n",
    "\n",
    "    logger.error(f\"Invalid summ response format: {response}\")\n",
    "    raise ValueError(f\"Invalid summ response format: {response}\")\n",
    "\n",
    "\n",
    "def extract_summ_acceptance(response: str) -> tuple[bool, str]:\n",
    "    pattern = r\"^(符合)(?:\\n乾淨版摘要：)?\\n(.+)|^(不符合)\"\n",
    "    match = re.match(pattern, response)\n",
    "    if match:\n",
    "        if match.group(1) is not None:\n",
    "            clean_summary = match.group(2)\n",
    "            return True, clean_summary\n",
    "        else:\n",
    "            return False, \"\"\n",
    "    else:\n",
    "        logger.error(f\"Invalid response format: {response}\")\n",
    "        return False, \"\"\n",
    "\n",
    "\n",
    "def format_summ(summ_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to format the data using the model.\n",
    "    \"\"\"\n",
    "    # Load the finished ids from the formatted file\n",
    "    finished_ids = get_finished_id(output_file)\n",
    "    logger.info(f\"Finished ids count: {len(finished_ids)}\")\n",
    "    logger.info(f\"Finished ids: {int_set_str(finished_ids)}\")\n",
    "\n",
    "    # Load the NewsWithRationale excluding the finished ids\n",
    "    summ_todo: list[dict] = read_summ_file(summ_file, finished_ids)\n",
    "    summ_finished: list[dict] = read_summ_file(output_file, finished_ids)\n",
    "\n",
    "    logger.info(f\"Loaded {len(summ_finished)} summ from {output_file}\")\n",
    "    logger.info(f\"Total {len(summ_todo)} summ to process\")\n",
    "\n",
    "    # summ_todo = summ_todo[:10]  # for demonstration\n",
    "\n",
    "    ids = [summ[\"id\"] for summ in summ_todo]\n",
    "    assert len(ids) == len(summ_todo)\n",
    "\n",
    "    logger.info(f\"Remains {len(ids)} summ to process\")\n",
    "    logger.info(f\"Remains summ ids: {int_set_str(set(ids))}\")\n",
    "\n",
    "    output_strs: list[str] = []\n",
    "    if USE_VLLM:\n",
    "        sys_prompt = format_summ_sys_prompt()\n",
    "        prompts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": format_summ_user_prompt(summ)}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            for summ in summ_todo\n",
    "        ]\n",
    "\n",
    "        assert len(prompts) == len(summ_todo)\n",
    "\n",
    "        # Filter out prompts that are too long\n",
    "        prompts, summ_todo = filter_by_max_length(\n",
    "            MAX_INPUT_LENGTH, prompts, summ_todo\n",
    "        )\n",
    "\n",
    "        responses = vllm_batch_generate(model, prompts, sampling_params)\n",
    "        output_strs = [response.outputs[0].text for response in responses]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Ollama processing not implemented\")\n",
    "\n",
    "    for i, (summ, output) in enumerate(zip(summ_todo, output_strs)):\n",
    "        try:\n",
    "            success, summ_str = extract_summ_acceptance(output)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing id {summ['id']}: {e}\")\n",
    "            logger.error(f\"Formatted response:\\n{output}\")\n",
    "            continue\n",
    "\n",
    "        if success:\n",
    "            # Save the formatted response\n",
    "            summ_finished.append(\n",
    "                {\n",
    "                    \"id\": summ[\"id\"],\n",
    "                    \"news\": summ[\"news\"],\n",
    "                    # \"response\": summ_str,\n",
    "                    \"response\": summ,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for summ in summ_finished:\n",
    "            f.write(json.dumps(summ, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b063a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(FORMAT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d12541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up invalid data\n",
    "\n",
    "finished_summ_ids = get_finished_id(SUMMARY_V3)\n",
    "\n",
    "finished_ids = finished_summ_ids.intersection(get_finished_id(ESSENTIALS_V3))\n",
    "\n",
    "\n",
    "summ_data: list[dict] = read_summ_file(SUMMARY_V3)\n",
    "ess_data: list[dict] = read_ess_tri_file(ESSENTIALS_V3)\n",
    "\n",
    "summ_data = [summ for summ in summ_data if summ[\"id\"] in finished_summ_ids]\n",
    "ess_data = [ess for ess in ess_data if ess[\"id\"] in finished_ids]\n",
    "\n",
    "print(f\"Finished summ ids count: {len(finished_summ_ids)}\")\n",
    "print(f\"Finished ids count: {len(finished_ids)}\")\n",
    "\n",
    "print(f\"Save {len(summ_data)} summ to {SUMMARY_V3}\")\n",
    "print(f\"Save {len(ess_data)} ess to {ESSENTIALS_V3}\")\n",
    "\n",
    "# with open(SUMMARY_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for summ in summ_data:\n",
    "#         f.write(json.dumps(summ, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# with open(ESSENTIALS_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for ess in ess_data:\n",
    "#         f.write(json.dumps(ess, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "formatted_summ_data: list[dict] = read_summ_file(SUMMARY_V3)\n",
    "summ_dict: dict[int, dict] = {summ[\"id\"]: summ for summ in formatted_summ_data}\n",
    "ess_dict = {ess[\"id\"]: ess for ess in ess_data}\n",
    "\n",
    "nwr_list: list[NWR] = []\n",
    "\n",
    "for id in finished_ids:\n",
    "    if id not in summ_dict or id not in ess_dict:\n",
    "        logger.warning(f\"Missing data for id {id}\")\n",
    "        continue\n",
    "\n",
    "    summ = extract_summ(summ_dict[id][\"response\"])\n",
    "    ess, tri = extract_essentials_and_triples(\n",
    "        ess_dict[id][\"response\"]\n",
    "    )\n",
    "\n",
    "    nwr = NWR(\n",
    "        article=summ_dict[id][\"news\"],\n",
    "        summary=summ,\n",
    "        id=id,\n",
    "        rationale_summary=summ,\n",
    "        essential_aspects=ess,\n",
    "        triples=tri,\n",
    "    )\n",
    "\n",
    "    nwr_list.append(nwr)\n",
    "\n",
    "logger.info(f\"Loaded {len(nwr_list)} NWRs\")\n",
    "\n",
    "# sort the NWRs by id\n",
    "nwr_list.sort(key=lambda x: x.id)\n",
    "\n",
    "with open(NWR_FORMATTED_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "    for nwr in nwr_list:\n",
    "        f.write(json.dumps(nwr.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "logger.info(f\"Saved {len(nwr_list)} NWRs to {NWR_FORMATTED_V3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_summ(\n",
    "    SUMMARY_V3,\n",
    "    SUMMARY_FORMATTED_V3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up invalid data\n",
    "\n",
    "# finished_summ_ids = get_finished_id(SUMMARY_FORMATTED_V3)\n",
    "# # print(f\"Finished summ ids count: {len(finished_summ_ids)}\")\n",
    "# finished_summ_ids = finished_summ_ids.intersection(get_finished_id(SUMMARY_V3))\n",
    "\n",
    "# finished_ids = finished_summ_ids.intersection(get_finished_id(ESSENTIALS_V3))\n",
    "\n",
    "\n",
    "# summ_data: list[dict] = read_summ_file(SUMMARY_V3)\n",
    "# ess_data: list[dict] = read_ess_tri_file(ESSENTIALS_V3)\n",
    "\n",
    "# summ_data = [summ for summ in summ_data if summ[\"id\"] in finished_summ_ids]\n",
    "# ess_data = [ess for ess in ess_data if ess[\"id\"] in finished_ids]\n",
    "\n",
    "# print(f\"Finished summ ids count: {len(finished_summ_ids)}\")\n",
    "# print(f\"Finished ids count: {len(finished_ids)}\")\n",
    "\n",
    "# print(f\"Save {len(summ_data)} summ to {SUMMARY_V3}\")\n",
    "# print(f\"Save {len(ess_data)} ess to {ESSENTIALS_V3}\")\n",
    "\n",
    "# # with open(SUMMARY_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "# #     for summ in summ_data:\n",
    "# #         f.write(json.dumps(summ, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# # with open(ESSENTIALS_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "# #     for ess in ess_data:\n",
    "# #         f.write(json.dumps(ess, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# formatted_summ_data: list[dict] = read_summ_file(SUMMARY_FORMATTED_V3)\n",
    "# summ_dict: dict[int, dict] = {summ[\"id\"]: summ for summ in formatted_summ_data}\n",
    "# ess_dict = {ess[\"id\"]: ess for ess in ess_data}\n",
    "\n",
    "# nwr_list: list[NWR] = []\n",
    "\n",
    "# for id in finished_ids:\n",
    "#     if id not in summ_dict or id not in ess_dict:\n",
    "#         logger.warning(f\"Missing data for id {id}\")\n",
    "#         continue\n",
    "\n",
    "#     ess, tri = extract_essentials_and_triples(\n",
    "#         ess_dict[id][\"response\"]\n",
    "#     )\n",
    "\n",
    "#     nwr = NWR(\n",
    "#         article=summ_dict[id][\"news\"],\n",
    "#         summary=summ_dict[id][\"response\"],\n",
    "#         id=id,\n",
    "#         rationale_summary=summ_dict[id][\"response\"],\n",
    "#         essential_aspects=ess,\n",
    "#         triples=tri,\n",
    "#     )\n",
    "\n",
    "#     nwr_list.append(nwr)\n",
    "\n",
    "# logger.info(f\"Loaded {len(nwr_list)} NWRs\")\n",
    "\n",
    "# # sort the NWRs by id\n",
    "# nwr_list.sort(key=lambda x: x.id)\n",
    "\n",
    "# with open(NWR_FORMATTED_V3, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for nwr in nwr_list:\n",
    "#         f.write(json.dumps(nwr.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# logger.info(f\"Saved {len(nwr_list)} NWRs to {NWR_FORMATTED_V3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27a198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
