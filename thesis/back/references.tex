
\renewcommand{\bibname}{References}

\begin{thebibliography}{99}

\bibitem{vaswani2017a} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. (2017a). Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

\bibitem{vaswani2017b} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. (2017b). Attention is all you need. Advances in neural information processing systems, 30.

\bibitem{trisum} Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han. (2024). TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale. arXiv preprint arXiv:2403.10351.

\bibitem{lora} Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.

\end{thebibliography}