{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from transformers import pipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from constants import DIR_STORED_DATA\n",
    "from crawler.utils import Logger\n",
    "from news_with_rationale import NewsWithRationale\n",
    "from rationale import Rationale\n",
    "from summarized_news import SummarizedNews\n",
    "from utils import *\n",
    "from xai import XAI\n",
    "\n",
    "\n",
    "logger = Logger(__name__)\n",
    "\n",
    "# DIR_STORED_DATA = 'stored_data'\n",
    "\n",
    "if not os.path.exists(DIR_STORED_DATA):\n",
    "    os.makedirs(DIR_STORED_DATA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"google/gemma-2-2b\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=\"meta-llama/Llama-3.2-1B\",\n",
    "#     torch_dtype=torch.bfloat16, \n",
    "#     # device_map=\"auto\"\n",
    "#     device=\"cuda\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate rationale by local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load news data\n",
    "\n",
    "from crawler.crawler_base import News, NewsCrawlerBase\n",
    "\n",
    "# NEWS_DIR = os.path.join(os.path.dirname(__file__), \"crawler/saved_news\")\n",
    "NEWS_DIR = \"crawler/saved_news\"\n",
    "\n",
    "def get_news_data() -> list[News]:\n",
    "    news: list[News] = []\n",
    "    news_data: list[tuple[str, str]] = [] # (url, file_path)\n",
    "    with open(os.path.join(NEWS_DIR, \"crawled_urls.json\"), \"r\") as f:\n",
    "        news_data = json.load(f)\n",
    "        # print(news_data)\n",
    "\n",
    "    file_list = [file for _, file in news_data]\n",
    "    for file in file_list:\n",
    "        news.append(NewsCrawlerBase._parse_file(file))\n",
    "    return news\n",
    "\n",
    "# news = get_news_data()\n",
    "# print(news[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rationale generation\n",
    "import time\n",
    "news = get_news_data()\n",
    "\n",
    "\n",
    "doc = news[0].content\n",
    "# print(f'{len(doc)=}')\n",
    "\n",
    "doc = doc.replace(\"\\n\\n\", \"\\n\")\n",
    "print(f'{len(doc)=}')\n",
    "# print(doc)\n",
    "\n",
    "# prompt = get_rationale_prompt_no_gt(doc)\n",
    "prompt = get_rationale_prompt_chinese(doc, \"test\")\n",
    "# print(f'{len(prompt)=}')\n",
    "\n",
    "\n",
    "s_t = time.time()\n",
    "output = pipe(prompt, max_length=2048)\n",
    "e_t = time.time()\n",
    "print(f'{e_t - s_t=}')\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert row data to SummarizedNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275596/275596 [13:57<00:00, 329.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 275596 summarized news data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert row data to SummarizedNews\n",
    "\n",
    "\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('s2twp')\n",
    "\n",
    "def process_str(s: str) -> str:\n",
    "    return cc.convert(s.replace(\" \", \"\"))  # translate to chinese traditional\n",
    "\n",
    "summarized_data: list[SummarizedNews] = []\n",
    "\n",
    "with open('CNewSum_v2/train.simple.label.jsonl', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    count = 0\n",
    "    # for line in lines:\n",
    "    for line in tqdm(lines):\n",
    "        data = json.loads(line)\n",
    "\n",
    "        article: list[str] = data['article']\n",
    "        article = [process_str(a) for a in article]\n",
    "        article_str = '\\n'.join(article)\n",
    "\n",
    "        summary: str = process_str(data['summary'])\n",
    "        id = data['id']\n",
    "        # assert len(data['label']) == 1\n",
    "        # label = data['label'][0]\n",
    "        label = data['label']\n",
    "        summarized_data.append(SummarizedNews(article_str, summary, id, label))\n",
    "\n",
    "\n",
    "print(f'read {len(summarized_data)} summarized news data')\n",
    "\n",
    "SummarizedNews.save_all(summarized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and generate rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SummarizedNews] [INFO] Loaded 275596 summarized news records\n",
      "[XAI] [INFO] XAI instance created\n",
      "[XAI] [INFO] loaded 2056 responses from x_ai responses\n",
      "\u001b[91m[XAI] [ERROR] failed to extract rationale from response: {'code': 'Some resource has been exhausted', 'error': 'h2 protocol error: http2 error'}\u001b[0m\n",
      "\u001b[91m[XAI] [ERROR] failed to extract rationale from response: {'code': 'Some resource has been exhausted', 'error': 'h2 protocol error: http2 error'}\u001b[0m\n",
      "\u001b[91m[XAI] [ERROR] failed to extract rationale from response: {'id': 'b6d8720b-d319-4324-8b50-2c0afb671e45', 'object': 'chat.completion', 'created': 1735325253, 'model': 'grok-beta', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '很抱歉，但您提供的摘要與文章內容不相符。摘要提到的是達沃斯論壇上的科技展示，而文章講述的是江蘇省鹽城市阜寧縣的自然災害和救災情況。請提供與文章內容相關的摘要，以便我能夠進行分析和摘要撰寫。', 'refusal': None}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 515, 'completion_tokens': 84, 'total_tokens': 599, 'prompt_tokens_details': {'text_tokens': 515, 'audio_tokens': 0, 'image_tokens': 0, 'cached_tokens': 0}}, 'system_fingerprint': 'fp_e1b909a5cb'}\u001b[0m\n",
      "[XAI] [INFO] loaded 2053 rationales from x_ai responses\n"
     ]
    }
   ],
   "source": [
    "summarized_news = SummarizedNews.load_all()\n",
    "\n",
    "x_ai = XAI()\n",
    "responses_from_x_ai = XAI.get_responses()\n",
    "rationales_from_x_ai = XAI.load_rationales_from_responses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 2000\n",
      "processing 2001\n",
      "processing 2002\n",
      "processing 2003\n",
      "processing 2004\n",
      "processing 2005\n",
      "processing 2006\n",
      "processing 2007\n",
      "processing 2008\n",
      "processing 2009\n",
      "processing 2010\n",
      "processing 2011\n",
      "processing 2012\n",
      "processing 2013\n",
      "processing 2014\n",
      "processing 2015\n",
      "processing 2016\n",
      "processing 2017\n",
      "processing 2018\n",
      "processing 2019\n",
      "processing 2020\n",
      "processing 2021\n",
      "processing 2022\n",
      "processing 2023\n",
      "processing 2024\n",
      "processing 2025\n",
      "processing 2026\n",
      "processing 2027\n",
      "processing 2028\n",
      "processing 2029\n",
      "processing 2030\n",
      "processing 2031\n",
      "processing 2032\n",
      "processing 2033\n",
      "processing 2034\n",
      "processing 2035\n",
      "processing 2036\n",
      "processing 2037\n",
      "processing 2038\n",
      "processing 2039\n",
      "processing 2040\n",
      "processing 2041\n",
      "processing 2042\n",
      "processing 2043\n",
      "processing 2044\n",
      "processing 2045\n",
      "processing 2046\n",
      "processing 2047\n",
      "processing 2048\n",
      "processing 2049\n",
      "processing 2050\n",
      "processing 2051\n",
      "processing 2052\n",
      "processing 2053\n",
      "processing 2054\n",
      "processing 2055\n",
      "processing 2056\n",
      "processing 2057\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m news \u001b[38;5;241m=\u001b[39m summarized_news[i]\n\u001b[1;32m----> 5\u001b[0m news_with_rationale \u001b[38;5;241m=\u001b[39m \u001b[43mXAI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_newsWithRationale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(news_with_rationale)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m news_with_rationale\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32mc:\\Users\\benny\\Desktop\\專題\\xai.py:127\u001b[0m, in \u001b[0;36mXAI.get_newsWithRationale\u001b[1;34m(cls, news)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_newsWithRationale\u001b[39m(\u001b[38;5;28mcls\u001b[39m, news: SummarizedNews) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NewsWithRationale:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mrationale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rationale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# return NewsWithRationale(news.article, news.summary, news.id, news.label, rationales)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NewsWithRationale(news, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mrationale)\n",
      "File \u001b[1;32mc:\\Users\\benny\\Desktop\\專題\\xai.py:120\u001b[0m, in \u001b[0;36mXAI.get_rationale\u001b[1;34m(news)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rationale\u001b[39m(news: SummarizedNews) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Rationale:\n\u001b[0;32m    119\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m get_rationale_prompt_chinese(news\u001b[38;5;241m.\u001b[39marticle, news\u001b[38;5;241m.\u001b[39msummary)\n\u001b[1;32m--> 120\u001b[0m     XAI\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m \u001b[43mXAI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     XAI\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mappend(XAI\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[0;32m    122\u001b[0m     XAI\u001b[38;5;241m.\u001b[39mstore_responses_expend(XAI\u001b[38;5;241m.\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\benny\\Desktop\\專題\\xai.py:61\u001b[0m, in \u001b[0;36mXAI.get_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     45\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     47\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     59\u001b[0m }\n\u001b[0;32m     60\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(data))\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\benny\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# corrupted = [472]\n",
    "for i in range(2000, 2300):\n",
    "    print(f\"processing {i}\")\n",
    "    news = summarized_news[i]\n",
    "    news_with_rationale = XAI.get_newsWithRationale(news)\n",
    "    # print(news_with_rationale)\n",
    "    news_with_rationale.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1512, label: [4, 5]\n",
      "article: 2015-01-1121:37。\n",
      "新浪體育。\n",
      "顯示圖片您的瀏覽器不支援video標籤。\n",
      "北京時間1月11日,CBA聯賽第31輪天津隊主場迎戰廣東隊。\n",
      "最終,廣東隊以126-114大勝利天津隊,豪取20連勝。\n",
      "朱芳雨得到30分5籃板。\n",
      "本場比賽廣東有6名球員得分上雙。\n",
      "朱芳雨三分球14中8得到30分。\n",
      "易建聯26分拜納姆24分。\n",
      "天津隊5名球員得分上雙。\n",
      "拜克斯拿下全隊最高的32分,張智涵30分。\n",
      "賽前廣東隊重要的防守大閘周鵬傷退回到廣東治療。\n",
      "王仕鵬和朱芳雨同時被提上首發讓廣東隊在一開始就衝擊力十足,第一節他們的三分球就14中9,其中朱芳雨一個人就投進4個三分球。\n",
      "在第一節的巨大優勢之後廣東隊便打的十分放鬆,縱然該場比賽天津隊偶有發揮,但面對多點開花的廣東隊,天津隊在主場輸掉了比賽。\n",
      "比賽的第一節好像就是廣東隊的隊內三分練習,半節過去朱芳雨就三分球5投4中,全隊更是投進了9個三分,他們的三分命中率甚至高於天津隊兩分的命中率。\n",
      "該場比賽廣東隊一共投進了17個三分,命中率高達61%!\n",
      "其中朱芳雨投進8個,王仕鵬三分球3投3中,拜納姆投進3個,高尚2個,易建聯也進了1個。\n",
      "今天廣東隊首發中鋒董瀚麟效率也是十分之高,他今天8投6中,罰球4罰3中得到15分9籃板。\n",
      "面對實力遜於自己的天津隊廣東的命中率十分之高兩分命中率高達53%。\n",
      "今天天津隊首次派出小將李榮培首發,這名95年的小將在首次首發之後打的十分積極,在首節他就發揮出色得到6分。\n",
      "他今天得到9分6助攻4籃板3搶斷,極好的彌補了張楠不在的遺憾。\n",
      "另一名錶現搶眼的是張驥,雖然他的資料不太搶眼,得到11分9籃板。\n",
      "但是在防守端他卻很好的限制了董瀚麟甚至易建聯,透過身體的接觸使兩名廣東內線十分生氣,出現了許多不該出現的失誤。\n",
      "國內球員張智涵今天十分積極,屢次下快攻得手,得到30分。\n",
      "天津隊今天發揮出了自己的水平,無奈對方實力更勝一籌。\n",
      "下場比賽廣東將主場迎戰同曦隊,而天津隊下場將客場挑戰八一隊。\n",
      "雙方首發:威廉姆斯。\n",
      "李榮培。\n",
      "張驥。\n",
      "張智涵。\n",
      "拜克斯廣東首發:朱芳雨。\n",
      "董瀚麟。\n",
      "王仕鵬。\n",
      "易建聯。\n",
      "拜納姆技術統計\n",
      "summary: CBA快訊:廣東以126-114大勝利天津,豪取20連勝,朱芳雨得到30分5籃板\n",
      "Essential Aspects:\n",
      "['比賽結果', '廣東隊的表現', '朱芳雨的表現']\n",
      "Triples:\n",
      "['[廣東隊 | 比賽結果 | 126-114大勝利天津隊]', '[廣東隊 | 連勝 | 20連勝]', '[朱芳雨 | 得分 | 30分]', '[朱芳雨 | 籃板 | 5籃板]', '[朱芳雨 | 三分球命中 | 14中8]']\n",
      "Summary:\n",
      "CBA聯賽中，廣東隊以126-114大勝利天津隊，取得了20連勝的佳績。朱芳雨在比賽中表現出色，得到30分和5個籃板，並且三分球14投8中。\n"
     ]
    }
   ],
   "source": [
    "# print(XAI.response)\n",
    "# print(XAI.rationale)\n",
    "# print(XAI.responses[-1])\n",
    "# print(summarized_news[1225].id)\n",
    "# print(XAI.extract_rationale(XAI.response))\n",
    "# print(NewsWithRationale(summarized_news[1224], XAI.extract_rationale(XAI.response)))\n",
    "# print(summarized_news[472])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "    \"id\": \"0f6f9b5a-ba7c-4b01-8054-aa28e855bbac\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1735058331,\n",
    "    \"model\": \"grok-beta\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"核心要素：\\n1. 海基會參訪大陸被叫停\\n2. 國臺辦的回應\\n3. 兩岸交流的影響\\n\\n三元組：\\n- [海基會 | 參訪被叫停 | 大陸]\\n- [國臺辦 | 回應 | 海基會參訪被叫停]\\n- [臺灣方面 | 原因 | 海基會取消來訪]\\n- [兩岸交流 | 受到干擾 | 海基會參訪被叫停]\\n\\n生成摘要：\\n國臺辦回應海基會參訪大陸被叫停，指出是因為臺灣方面的原因，導致海基會取消了這次來訪。我們不希望兩岸正常的交流交往受到干擾。\",\n",
    "                \"refusal\": None\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 657,\n",
    "        \"completion_tokens\": 182,\n",
    "        \"total_tokens\": 839,\n",
    "        \"prompt_tokens_details\": {\n",
    "            \"text_tokens\": 657,\n",
    "            \"audio_tokens\": 0,\n",
    "            \"image_tokens\": 0,\n",
    "            \"cached_tokens\": 0\n",
    "        }\n",
    "    },\n",
    "    \"system_fingerprint\": \"fp_efe33e0791\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# news_with_rationale = NewsWithRationale(summarized_news[201], XAI.extract_rationale(XAI.response))\n",
    "# # print(news_with_rationale.__str__())\n",
    "# print(news_with_rationale.__dict__)\n",
    "# news_with_rationale.save()\n",
    "\n",
    "# XAI.response = {\"id\": \"e33efe41-e762-4f4d-9174-b1c8ce276b42\", \"object\": \"chat.completion\", \"created\": 1735313831, \"model\": \"grok-beta\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"核心要素：\\n1. 中國和印度的軍工行業發展\\n2. 中國軍工行業的效率和先進水平\\n3. 印度軍工行業的發展狀況\\n\\n三元組：\\n\\n[中國 | 軍工行業發展 | 遠超印度]\\n[中國軍工行業 | 效率 | 先進水平的象徵]\\n[印度軍工行業 | 發展狀況 | 效率低下]\\n[中國 | 軍事研發 | 成功]\\n[中國 | 國防預算 | 穩步增加]\\n[中國軍工企業 | 參展 | 蘭卡威國際海事及航空航天展]\\n\\n生成摘要：\\n美媒稱，中國軍工發展水平遠超印度。與印度相比，中國軍工行業是效率和先進水平的象徵。中國在軍事研發方面取得了成功，並通過穩步增加國防預算來支持軍工行業的現代化。相比之下，印度軍工行業的發展狀況顯得效率低下。中國軍工企業還積極參加了蘭卡威國際海事及航空航天展，展示其精良裝備。\", \"refusal\": False}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 1259, \"completion_tokens\": 284, \"total_tokens\": 1543, \"prompt_tokens_details\": {\"text_tokens\": 1259, \"audio_tokens\": 0, \"image_tokens\": 0, \"cached_tokens\": 0}}, \"system_fingerprint\": \"fp_e1b909a5cb\"}\n",
    "\n",
    "# # print(XAI.response)\n",
    "# rationale = XAI.extract_rationale(XAI.response)\n",
    "# print(rationale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test local rationale generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model_from_pyTorch(model_name: str):\n",
    "    # load the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_text_from_model(tokenizer, model, prompt, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "news = get_news_data()\n",
    "doc = news[0].content\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "tokenizer, model = load_model_from_pyTorch(model_name)\n",
    "print(f'{model.device=}')\n",
    "prompt = get_rationale_prompt_no_gt(doc)\n",
    "print(f'{len(prompt)=}')\n",
    "output = generate_text_from_model(tokenizer, model, prompt, max_length=3000)\n",
    "print(f'{len(output)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.24038430538092\n"
     ]
    }
   ],
   "source": [
    "def f(p: float) -> float:\n",
    "    return 20000 / (p * 200 + (1 - p) * 40)\n",
    "\n",
    "print(f(0.485)*40/211.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "給定一份文章及其摘要，完成以下任務：\n",
      "(1) 根據摘要，提取文章的核心要素。\n",
      "(2) 對於每個核心要素，檢索詳細的三元組，格式為 [實體1 | 關係 | 實體2]，這些三元組用於構成真實摘要。\n",
      "(3) 使用檢索到的三元組撰寫一份摘要。核心要素、三元組和撰寫的摘要應該在同一份回應中，並以換行符分隔。所有三元組 [實體1 | 關係 | 實體2] 的長度必須為3（以 \"|\" 分隔）。\n",
      "範例：\n",
      "================範例=================\n",
      "提示：\n",
      "[文件]: [文件]\n",
      "[摘要]: [摘要]\n",
      "更新：\n",
      "核心要素：\n",
      "[核心要素]\n",
      "三元組：\n",
      "\n",
      "[實體1_1 | 關係_1 | 實體1_2]\n",
      "[實體2_1 | 關係_2 | 實體2_2]\n",
      "[實體3_1 | 關係_3 | 實體3_2]\n",
      "...\n",
      "生成摘要：\n",
      "[摘要]\n",
      "========================================\n",
      "提示：\n",
      "[文件]: 大象給小朋友戴禮帽。\n",
      "距離“六一”國際兒童節還有幾天,錫城園林景區已經準備了豐富的活動讓孩子度過一個歡樂的節日。\n",
      "其中錫惠景區將放飛五彩繽紛的蝴蝶,無錫動物園海洋館將邀請“美人魚”與海底“藍精靈”共舞,部分景區在兒童節期間還對孩子或陪同家長實行門票減免的優惠措施。\n",
      "昨日,適逢無錫動物園小象茜茜的9歲生日。\n",
      "動物園邀請幾個家庭和小象一起度過愉快的生日派對。\n",
      "如今,小象身高已經2.2米,體重更達1.8噸。\n",
      "為此動物園為小象製作了一個5層近2米高的水果青草蛋糕。\n",
      "在高高的生日蛋糕面前,孩子們用西瓜、蘋果、香蕉、南瓜、西紅柿等蔬菜水果將這5層蛋糕精心裝點一番。\n",
      "孩子們為小象茜茜唱了生日歌,還親手餵它吃了水果青草蛋糕。\n",
      "作為回禮,小象茜茜不僅表演了拿手的絕活——小象圓舞曲,還用它長長的鼻子為孩子們戴禮帽。\n",
      "表演完畢後,小象茜茜好好地洗了個澡,還跟在場的孩子們玩了象鼻噴水遊戲。\n",
      "據瞭解,“六一”節期間,無錫海洋館將邀請美麗的俄羅斯“美人魚”與可愛的海底“藍精靈”共舞,5月30日到6月1日每天表演2場。\n",
      "5月30日到6。\n",
      "月7日,錫惠名勝區內中央大廳前的夢幻蝴蝶園將放飛各式各樣的蝴蝶,5月30日到6月1日的上午10點,進入夢幻蝴蝶園的前61名孩子們可以體驗到自己放飛蝴蝶的樂趣。\n",
      "5月30日到6月7日,錫惠名勝區荷軒長廊內將展出以“多彩童年·愛的憧憬”為主題的蝶戀花少兒繪畫大賽獲獎作品。\n",
      "園林景區在節日期間還推出很多門票優惠政策。\n",
      "如動物園5月30日到6月1日三天,凡14週歲以下或者身高1.5米以下少年兒童憑有效證件可免票遊玩無錫動物園和無錫海洋館。\n",
      "錫惠景區5月30日到6月1日期間,凡是18週歲以下的少年兒童進入錫惠景區及惠山古鎮遊覽,陪同的家長可享受門票半價優惠,一名少年兒童限優惠兩名家長。\n",
      "編輯:沈偉\n",
      "[摘要]: 無錫園林景區“六一”活動多:部分景區實行門票減免,1.5米以下兒童可免費遊覽海洋館\n",
      "更新：\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = get_rationale_prompt_chinese(summarized_news[2058].article, summarized_news[2058].summary)\n",
    "print(prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
