{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from transformers import pipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from constants import DIR_STORED_DATA\n",
    "from crawler.utils import Logger\n",
    "from news_with_rationale import NewsWithRationale\n",
    "from rationale import Rationale\n",
    "from summarized_news import SummarizedNews\n",
    "from utils import *\n",
    "from xai import XAI\n",
    "\n",
    "\n",
    "logger = Logger(__name__)\n",
    "\n",
    "# DIR_STORED_DATA = 'stored_data'\n",
    "\n",
    "if not os.path.exists(DIR_STORED_DATA):\n",
    "    os.makedirs(DIR_STORED_DATA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"google/gemma-2-2b\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     device=\"cuda\",\n",
    "# )\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=\"meta-llama/Llama-3.2-1B\",\n",
    "#     torch_dtype=torch.bfloat16, \n",
    "#     # device_map=\"auto\"\n",
    "#     device=\"cuda\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate rationale by local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load news data\n",
    "\n",
    "from crawler.crawler_base import News, NewsCrawlerBase\n",
    "\n",
    "# NEWS_DIR = os.path.join(os.path.dirname(__file__), \"crawler/saved_news\")\n",
    "NEWS_DIR = \"crawler/saved_news\"\n",
    "\n",
    "def get_news_data() -> list[News]:\n",
    "    news: list[News] = []\n",
    "    news_data: list[tuple[str, str]] = [] # (url, file_path)\n",
    "    with open(os.path.join(NEWS_DIR, \"crawled_urls.json\"), \"r\") as f:\n",
    "        news_data = json.load(f)\n",
    "        # print(news_data)\n",
    "\n",
    "    file_list = [file for _, file in news_data]\n",
    "    for file in file_list:\n",
    "        news.append(NewsCrawlerBase._parse_file(file))\n",
    "    return news\n",
    "\n",
    "# news = get_news_data()\n",
    "# print(news[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rationale generation\n",
    "import time\n",
    "news = get_news_data()\n",
    "\n",
    "\n",
    "doc = news[0].content\n",
    "# print(f'{len(doc)=}')\n",
    "\n",
    "doc = doc.replace(\"\\n\\n\", \"\\n\")\n",
    "print(f'{len(doc)=}')\n",
    "# print(doc)\n",
    "\n",
    "# prompt = get_rationale_prompt_no_gt(doc)\n",
    "prompt = get_rationale_prompt_chinese(doc, \"test\")\n",
    "# print(f'{len(prompt)=}')\n",
    "\n",
    "\n",
    "s_t = time.time()\n",
    "output = pipe(prompt, max_length=2048)\n",
    "e_t = time.time()\n",
    "print(f'{e_t - s_t=}')\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert row data to SummarizedNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275596/275596 [13:57<00:00, 329.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 275596 summarized news data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert row data to SummarizedNews\n",
    "\n",
    "\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('s2twp')\n",
    "\n",
    "def process_str(s: str) -> str:\n",
    "    return cc.convert(s.replace(\" \", \"\"))  # translate to chinese traditional\n",
    "\n",
    "summarized_data: list[SummarizedNews] = []\n",
    "\n",
    "with open('CNewSum_v2/train.simple.label.jsonl', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    count = 0\n",
    "    # for line in lines:\n",
    "    for line in tqdm(lines):\n",
    "        data = json.loads(line)\n",
    "\n",
    "        article: list[str] = data['article']\n",
    "        article = [process_str(a) for a in article]\n",
    "        article_str = '\\n'.join(article)\n",
    "\n",
    "        summary: str = process_str(data['summary'])\n",
    "        id = data['id']\n",
    "        # assert len(data['label']) == 1\n",
    "        # label = data['label'][0]\n",
    "        label = data['label']\n",
    "        summarized_data.append(SummarizedNews(article_str, summary, id, label))\n",
    "\n",
    "\n",
    "print(f'read {len(summarized_data)} summarized news data')\n",
    "\n",
    "SummarizedNews.save_all(summarized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and generate rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SummarizedNews] [INFO] Loaded 275596 summarized news records\n",
      "[XAI] [INFO] XAI instance created\n",
      "[XAI] [INFO] loaded 207 responses from x_ai responses\n",
      "[XAI] [INFO] loaded 207 rationales from x_ai responses\n"
     ]
    }
   ],
   "source": [
    "summarized_news = SummarizedNews.load_all()\n",
    "\n",
    "x_ai = XAI()\n",
    "responses_from_x_ai = XAI.get_responses()\n",
    "rationales_from_x_ai = XAI.load_rationales_from_responses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 210\n",
      "processing 211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(210, 212):\n",
    "    print(f\"processing {i}\")\n",
    "    news = summarized_news[i]\n",
    "    news_with_rationale = XAI.get_newsWithRationale(news)\n",
    "    # print(news_with_rationale)\n",
    "    news_with_rationale.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "    \"id\": \"0f6f9b5a-ba7c-4b01-8054-aa28e855bbac\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1735058331,\n",
    "    \"model\": \"grok-beta\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"核心要素：\\n1. 海基會參訪大陸被叫停\\n2. 國臺辦的回應\\n3. 兩岸交流的影響\\n\\n三元組：\\n- [海基會 | 參訪被叫停 | 大陸]\\n- [國臺辦 | 回應 | 海基會參訪被叫停]\\n- [臺灣方面 | 原因 | 海基會取消來訪]\\n- [兩岸交流 | 受到干擾 | 海基會參訪被叫停]\\n\\n生成摘要：\\n國臺辦回應海基會參訪大陸被叫停，指出是因為臺灣方面的原因，導致海基會取消了這次來訪。我們不希望兩岸正常的交流交往受到干擾。\",\n",
    "                \"refusal\": None\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 657,\n",
    "        \"completion_tokens\": 182,\n",
    "        \"total_tokens\": 839,\n",
    "        \"prompt_tokens_details\": {\n",
    "            \"text_tokens\": 657,\n",
    "            \"audio_tokens\": 0,\n",
    "            \"image_tokens\": 0,\n",
    "            \"cached_tokens\": 0\n",
    "        }\n",
    "    },\n",
    "    \"system_fingerprint\": \"fp_efe33e0791\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# news_with_rationale = NewsWithRationale(summarized_news[201], XAI.extract_rationale(XAI.response))\n",
    "# # print(news_with_rationale.__str__())\n",
    "# print(news_with_rationale.__dict__)\n",
    "# news_with_rationale.save()\n",
    "\n",
    "# XAI.response = {\"id\": \"e33efe41-e762-4f4d-9174-b1c8ce276b42\", \"object\": \"chat.completion\", \"created\": 1735313831, \"model\": \"grok-beta\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"核心要素：\\n1. 中國和印度的軍工行業發展\\n2. 中國軍工行業的效率和先進水平\\n3. 印度軍工行業的發展狀況\\n\\n三元組：\\n\\n[中國 | 軍工行業發展 | 遠超印度]\\n[中國軍工行業 | 效率 | 先進水平的象徵]\\n[印度軍工行業 | 發展狀況 | 效率低下]\\n[中國 | 軍事研發 | 成功]\\n[中國 | 國防預算 | 穩步增加]\\n[中國軍工企業 | 參展 | 蘭卡威國際海事及航空航天展]\\n\\n生成摘要：\\n美媒稱，中國軍工發展水平遠超印度。與印度相比，中國軍工行業是效率和先進水平的象徵。中國在軍事研發方面取得了成功，並通過穩步增加國防預算來支持軍工行業的現代化。相比之下，印度軍工行業的發展狀況顯得效率低下。中國軍工企業還積極參加了蘭卡威國際海事及航空航天展，展示其精良裝備。\", \"refusal\": False}, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 1259, \"completion_tokens\": 284, \"total_tokens\": 1543, \"prompt_tokens_details\": {\"text_tokens\": 1259, \"audio_tokens\": 0, \"image_tokens\": 0, \"cached_tokens\": 0}}, \"system_fingerprint\": \"fp_e1b909a5cb\"}\n",
    "\n",
    "# # print(XAI.response)\n",
    "# rationale = XAI.extract_rationale(XAI.response)\n",
    "# print(rationale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test local rationale generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model_from_pyTorch(model_name: str):\n",
    "    # load the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_text_from_model(tokenizer, model, prompt, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "news = get_news_data()\n",
    "doc = news[0].content\n",
    "\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "tokenizer, model = load_model_from_pyTorch(model_name)\n",
    "print(f'{model.device=}')\n",
    "prompt = get_rationale_prompt_no_gt(doc)\n",
    "print(f'{len(prompt)=}')\n",
    "output = generate_text_from_model(tokenizer, model, prompt, max_length=3000)\n",
    "print(f'{len(output)=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
